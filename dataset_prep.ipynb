{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "from models import OmniwardModel\n",
    "from data import RaceDataset\n",
    "\n",
    "from racing_utils.utils import closest_point_idx, cyclic_slice, determine_direction_of_bound, rotate_into_map_coord\n",
    "from racing_utils.torch_related import TensorStandardScaler, calc_reward_and_penalty, scale_batch_and_to_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.backends.cudnn.determinstic = True\n",
    "# torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_STEPS_AHEAD_TRAJ = 150\n",
    "NUM_STEPS_AHEAD_ACT = 10\n",
    "NUM_STEPS_AHEAD_BOUND = 50\n",
    "NUM_STEPS_CENTERLINE = 300\n",
    "CENTERLINE_DECIMATION = 1\n",
    "PATH_TO_F1TENTH_GYM = Path('../f1tenth_gym')\n",
    "FULL_STATE_FOR_TRAJECTORY = False\n",
    "\n",
    "DATASET_SUFFIX = '_tiny' # '_tiny' or ''\n",
    "\n",
    "DEVICE = 'cuda:0' # 'cuda:0'\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "PROB_FLIP = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpickled = pd.read_pickle('./data/valid_tiny/2021-10-25_12_36_28.612196.pkl')\n",
    "one_race = unpickled['data']\n",
    "additional_data = unpickled['additional_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centerline = additional_data['centerline'][::CENTERLINE_DECIMATION]\n",
    "lookahead_distance = additional_data['lookahead_distance']\n",
    "speed_setpoint = additional_data['speed_setpoint']\n",
    "tire_force_max = additional_data['tire_force_max']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_race.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We need to figure out if the bounds go in the same direction as the car is driving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_position, end_position = one_race.loc[[0, NUM_STEPS_AHEAD_TRAJ], 'position']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = []\n",
    "bound_directions = []\n",
    "for csv_file in ['interior.csv', 'exterior.csv']:\n",
    "    bound = pd.read_csv(PATH_TO_F1TENTH_GYM / 'maps' / csv_file, header=None).values\n",
    "    direction = determine_direction_of_bound(bound, start_position, end_position)\n",
    "    bound_directions.append(direction)\n",
    "    bound = bound[::direction]\n",
    "    bounds.append(bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_id = 120 + len(one_race) // 4\n",
    "row = one_race.iloc[row_id]\n",
    "\n",
    "position = row['position']\n",
    "yaw = row['yaw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_bound_indices = [closest_point_idx(position, bound) for bound in bounds] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bound_slices = [\n",
    "    cyclic_slice(bound, closest_idx, NUM_STEPS_AHEAD_BOUND)\n",
    "    for bound, closest_idx in zip(bounds, closest_bound_indices)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_positions = np.stack(one_race['position'].iloc[row_id:row_id+NUM_STEPS_AHEAD_TRAJ].values)\n",
    "\n",
    "for bound_slice in bound_slices:\n",
    "    plt.scatter(bound_slice[:, 0], bound_slice[:, 1])\n",
    "    \n",
    "plt.scatter(local_positions[:, 0], local_positions[:, 1])\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_positions = np.stack(one_race['position'].iloc[row_id:row_id+NUM_STEPS_AHEAD_TRAJ].values)\n",
    "\n",
    "closest_centerline_idx = closest_point_idx(local_positions[0], centerline)\n",
    "centerline_ahead = cyclic_slice(centerline, closest_centerline_idx, NUM_STEPS_CENTERLINE)\n",
    "centerline_ahead = rotate_into_map_coord(centerline_ahead - position, -yaw)\n",
    "plt.scatter(centerline_ahead[:, 0], centerline_ahead[:, 1], color='r', alpha=0.2)\n",
    "\n",
    "for bound_slice in bound_slices:\n",
    "    bound_slice = rotate_into_map_coord(bound_slice - position, -yaw)\n",
    "    plt.scatter(bound_slice[:, 0], bound_slice[:, 1], color='gray', alpha=0.2)\n",
    "        \n",
    "local_positions = rotate_into_map_coord(local_positions - position, -yaw)\n",
    "plt.scatter(local_positions[:, 0], local_positions[:, 1], alpha=0.1)\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = RaceDataset(\n",
    "    NUM_STEPS_AHEAD_TRAJ,\n",
    "    NUM_STEPS_AHEAD_ACT,\n",
    "    NUM_STEPS_AHEAD_BOUND,\n",
    "    NUM_STEPS_CENTERLINE,\n",
    "    f'./data/train{DATASET_SUFFIX}',\n",
    "    CENTERLINE_DECIMATION,\n",
    "    prob_flip=PROB_FLIP,\n",
    "    full_state_for_trajectory=FULL_STATE_FOR_TRAJECTORY,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, targets = train_dataset[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_bound, right_bound = features['left_bound'], features['right_bound']\n",
    "left_bound = left_bound.reshape(-1, 2)\n",
    "right_bound = right_bound.reshape(-1, 2)\n",
    "plt.scatter(left_bound[:, 0], left_bound[:, 1], color='gray', alpha=0.2)\n",
    "plt.scatter(right_bound[:, 0], right_bound[:, 1], color='gray', alpha=0.2)\n",
    "\n",
    "centerline = features['centerline'].reshape(-1, 2)\n",
    "plt.scatter(centerline[:, 0], centerline[:, 1], color='r', alpha=0.2)\n",
    "\n",
    "trajectory = targets['trajectory']\n",
    "trajectory = trajectory.reshape(-1, 2)\n",
    "plt.scatter(trajectory[:, 0], trajectory[:, 1], color='blue', alpha=0.1)\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = data.DataLoader(train_dataset, BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Initilize the scalers\n",
    "features_scalers = {key: TensorStandardScaler(DEVICE) for key in features.keys()}\n",
    "targets_scalers = {key: TensorStandardScaler(DEVICE) for key in targets.keys()}\n",
    "\n",
    "# Do a partial fit of the scalers for both the input features, and the targets\n",
    "for features_batch, targets_batch in train_loader:\n",
    "    for feature_name in features.keys():\n",
    "        features_scalers[feature_name].partial_fit(features_batch[feature_name])\n",
    "\n",
    "    for target_name in targets.keys():\n",
    "        targets_scalers[target_name].partial_fit(targets_batch[target_name])\n",
    "\n",
    "\n",
    "for feature_name in features.keys():\n",
    "        features_scalers[feature_name].tensorfy()\n",
    "\n",
    "for target_name in targets.keys():\n",
    "    targets_scalers[target_name].tensorfy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = RaceDataset(\n",
    "    NUM_STEPS_AHEAD_TRAJ,\n",
    "    NUM_STEPS_AHEAD_ACT,\n",
    "    NUM_STEPS_AHEAD_BOUND,\n",
    "    NUM_STEPS_CENTERLINE,\n",
    "    f'./data/valid{DATASET_SUFFIX}',\n",
    "    CENTERLINE_DECIMATION,\n",
    "    prob_flip=PROB_FLIP,\n",
    "    full_state_for_trajectory=FULL_STATE_FOR_TRAJECTORY,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = len(features['state'])\n",
    "contr_params_size = len(features['contr_params'])\n",
    "centerline_size = len(features['centerline'])\n",
    "trajectory_size = len(targets['trajectory'])\n",
    "actuators_size = len(targets['speeds_and_deltas'])\n",
    "\n",
    "num_stacked = 3\n",
    "centerline_encoder_sizes = num_stacked * [centerline_size // 4]\n",
    "middle_sizes = num_stacked * [(centerline_size + trajectory_size + actuators_size) // 4]\n",
    "output_sizes = [centerline_size // 2, (trajectory_size + actuators_size) // 4, (trajectory_size + actuators_size) // 2]\n",
    "\n",
    "\n",
    "omniward_model = OmniwardModel(\n",
    "    state_size,\n",
    "    contr_params_size,\n",
    "    centerline_size,\n",
    "    centerline_encoder_sizes,\n",
    "    middle_sizes,\n",
    "    output_sizes,\n",
    "    trajectory_size,\n",
    "    actuators_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omniward_model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = data.DataLoader(train_dataset, BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "valid_loader = data.DataLoader(valid_dataset, BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory_gamma = 0.99999\n",
    "actuators_gamma = 0.8\n",
    "\n",
    "trajectory_gammas = (trajectory_gamma ** np.r_[np.arange(trajectory_size // 2), np.arange(trajectory_size // 2)])\n",
    "trajectory_gammas = torch.from_numpy(trajectory_gammas[::-1].copy()).to(DEVICE)\n",
    "\n",
    "actuators_gammas = actuators_gamma ** np.r_[np.arange(actuators_size // 2), np.arange(actuators_size // 2)]\n",
    "actuators_gammas = torch.from_numpy(actuators_gammas).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traj_act_mse(*, traj_coeff, act_coeff, traj_gammas, act_gammas, traj, traj_pred, act, act_pred):\n",
    "    return (\n",
    "        traj_coeff * (traj_gammas * (traj - traj_pred) ** 2).mean(),\n",
    "        act_coeff * (act_gammas * (act - act_pred) ** 2).mean(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PENALTY_SIGMA = 0.15\n",
    "\n",
    "trajectory_mse_coeff = 1.0\n",
    "actuators_mse_coeff = 1.0\n",
    "\n",
    "optimizers = (\n",
    "    [5, torch.optim.Adam(omniward_model.parameters(), lr=1e-3)],\n",
    "    [10, torch.optim.Adam(omniward_model.parameters(), lr=1e-4)],\n",
    "    [2, torch.optim.Adam(omniward_model.parameters(), lr=1e-5)],\n",
    ")\n",
    "train_traj_mses_for_plot = []\n",
    "train_act_mses_for_plot = []\n",
    "train_reward_mses_for_plot = []\n",
    "train_penalty_mses_for_plot = []\n",
    "\n",
    "valid_traj_mses_for_plot = []\n",
    "valid_act_mses_for_plot = []\n",
    "valid_reward_mses_for_plot = []\n",
    "valid_penalty_mses_for_plot = []\n",
    "\n",
    "epoch = 0\n",
    "for num_epochs_per_optimizer_round, optimizer in optimizers:\n",
    "    print(f'Optimizer: {optimizer}')\n",
    "    for _ in range(num_epochs_per_optimizer_round):\n",
    "        \n",
    "        ############\n",
    "        # Training #\n",
    "        ############\n",
    "        train_traj_mse = 0\n",
    "        train_act_mse = 0\n",
    "        train_reward_mse = 0\n",
    "        train_penalty_mse = 0\n",
    "        omniward_model.train()\n",
    "        for features_batch, targets_batch in train_loader:\n",
    "            centerline = features_batch['centerline'].to(DEVICE)\n",
    "            left_bound = features_batch['left_bound'].to(DEVICE)\n",
    "            right_bound = features_batch['right_bound'].to(DEVICE)\n",
    "            trajectory = targets_batch['trajectory'].to(DEVICE)\n",
    "\n",
    "            reward, penalty = calc_reward_and_penalty(trajectory, centerline, left_bound, right_bound, penalty_sigma=PENALTY_SIGMA)\n",
    "            \n",
    "            features_batch, targets_batch = scale_batch_and_to_device(DEVICE, features_scalers, targets_scalers, features_batch, targets_batch)\n",
    "            \n",
    "            preds = omniward_model(**features_batch)\n",
    "            trajectory_pred, actuators_pred = preds['trajectory_pred'], preds['actuators_pred']\n",
    "            \n",
    "            trajectory = targets_batch['trajectory'].squeeze(dim=1)\n",
    "            actuators = targets_batch['speeds_and_deltas'].squeeze(dim=1)\n",
    "            \n",
    "            traj_mse, act_mse = traj_act_mse(\n",
    "                traj_coeff=trajectory_mse_coeff,\n",
    "                act_coeff=actuators_mse_coeff,\n",
    "                traj_gammas=trajectory_gammas,\n",
    "                act_gammas=actuators_gammas,\n",
    "                traj=trajectory,\n",
    "                traj_pred=trajectory_pred,\n",
    "                act=actuators,\n",
    "                act_pred=actuators_pred,\n",
    "            )\n",
    "                    \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            (traj_mse + act_mse).backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "\n",
    "            train_traj_mse += float(traj_mse)\n",
    "            train_act_mse += float(act_mse)\n",
    "\n",
    "            trajectory_pred = targets_scalers['trajectory'].inverse_transform(trajectory_pred)\n",
    "            reward_pred, penalty_pred = calc_reward_and_penalty(trajectory_pred, centerline, left_bound, right_bound, penalty_sigma=PENALTY_SIGMA)\n",
    "            train_reward_mse += float(((reward - reward_pred) ** 2).mean())\n",
    "            train_penalty_mse += float(((penalty - penalty_pred) ** 2).mean())\n",
    "\n",
    "        avg_train_traj_mse = train_traj_mse / len(train_loader)\n",
    "        avg_train_act_mse = train_act_mse / len(train_loader)\n",
    "        avg_train_reward_mse = train_reward_mse / len(train_loader)\n",
    "        avg_train_penalty_mse = train_penalty_mse / len(train_loader)\n",
    "        print(f'\\n{epoch}: train_traj_MSE: {avg_train_traj_mse:.3f}, train_act_MSE: {avg_train_act_mse:.3f}, train_reward_MSE: {avg_train_reward_mse:.3f}, train_penalty_MSE: {avg_train_penalty_mse:.3f}')\n",
    "        train_traj_mses_for_plot.append(avg_train_traj_mse)\n",
    "        train_act_mses_for_plot.append(avg_train_act_mse)\n",
    "        train_reward_mses_for_plot.append(avg_train_reward_mse)\n",
    "        train_penalty_mses_for_plot.append(avg_train_penalty_mse)\n",
    "        \n",
    "            \n",
    "        ##############\n",
    "        # Validation #\n",
    "        ##############\n",
    "        valid_traj_mse = 0\n",
    "        valid_act_mse = 0\n",
    "        valid_reward_mse = 0\n",
    "        valid_penalty_mse = 0\n",
    "        omniward_model.eval()\n",
    "        for features_batch, targets_batch in valid_loader:\n",
    "            centerline = features_batch['centerline'].to(DEVICE)\n",
    "            left_bound = features_batch['left_bound'].to(DEVICE)\n",
    "            right_bound = features_batch['right_bound'].to(DEVICE)\n",
    "            trajectory = targets_batch['trajectory'].to(DEVICE)\n",
    "\n",
    "            reward, penalty = calc_reward_and_penalty(trajectory, centerline, left_bound, right_bound, penalty_sigma=PENALTY_SIGMA)\n",
    "\n",
    "            features_batch, targets_batch = scale_batch_and_to_device(DEVICE, features_scalers, targets_scalers, features_batch, targets_batch)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                preds = omniward_model(**features_batch)\n",
    "                trajectory_pred, actuators_pred = preds['trajectory_pred'], preds['actuators_pred']\n",
    "\n",
    "            trajectory = targets_batch['trajectory'].squeeze(dim=1)\n",
    "            actuators = targets_batch['speeds_and_deltas'].squeeze(dim=1)\n",
    "            \n",
    "            traj_mse, act_mse = traj_act_mse(\n",
    "                traj_coeff=trajectory_mse_coeff,\n",
    "                act_coeff=actuators_mse_coeff,\n",
    "                traj_gammas=trajectory_gammas,\n",
    "                act_gammas=actuators_gammas,\n",
    "                traj=trajectory,\n",
    "                traj_pred=trajectory_pred,\n",
    "                act=actuators,\n",
    "                act_pred=actuators_pred,\n",
    "            )\n",
    "                    \n",
    "            valid_traj_mse += float(traj_mse)\n",
    "            valid_act_mse += float(act_mse)\n",
    "\n",
    "            trajectory_pred = targets_scalers['trajectory'].inverse_transform(trajectory_pred)\n",
    "            reward_pred, penalty_pred = calc_reward_and_penalty(trajectory_pred, centerline, left_bound, right_bound, penalty_sigma=PENALTY_SIGMA)\n",
    "            valid_reward_mse += float(((reward - reward_pred) ** 2).mean())\n",
    "            valid_penalty_mse += float(((penalty - penalty_pred) ** 2).mean())\n",
    "            \n",
    "        avg_valid_traj_mse = valid_traj_mse / len(valid_loader)\n",
    "        avg_valid_act_mse = valid_act_mse / len(valid_loader)\n",
    "        avg_valid_reward_mse = valid_reward_mse / len(valid_loader)\n",
    "        avg_valid_penalty_mse = valid_penalty_mse / len(valid_loader)\n",
    "        print(f'{epoch}: valid_traj_MSE: {avg_valid_traj_mse:.3f}, valid_act_MSE: {avg_valid_act_mse:.3f}, valid_reward_MSE: {avg_valid_reward_mse:.3f}, valid_penalty_MSE: {avg_valid_penalty_mse:.3f}')\n",
    "        valid_traj_mses_for_plot.append(avg_valid_traj_mse)\n",
    "        valid_act_mses_for_plot.append(avg_valid_act_mse)\n",
    "        valid_reward_mses_for_plot.append(avg_valid_reward_mse)\n",
    "        valid_penalty_mses_for_plot.append(avg_valid_penalty_mse)\n",
    "\n",
    "        epoch += 1\n",
    "\n",
    "    plt.scatter(penalty.detach().cpu().numpy(), penalty_pred.detach().cpu().numpy(), alpha=0.15)\n",
    "    plt.show()\n",
    "    plt.scatter(reward.detach().cpu().numpy(), reward_pred.detach().cpu().numpy(), alpha=0.15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "with torch.inference_mode():\n",
    "    preds = omniward_model(**features_batch)\n",
    "    dupa, dupa2 = preds['trajectory_pred'], preds['actuators_pred']\n",
    "    dupa = targets_scalers['trajectory'].inverse_transform(dupa)\n",
    "    dupa, dupa2 = calc_reward_and_penalty(dupa, centerline, left_bound, right_bound, penalty_sigma=PENALTY_SIGMA)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_traj_mses_for_plot, label='train_traj_mse')\n",
    "plt.plot(valid_traj_mses_for_plot, label='valid_traj_mse')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(train_act_mses_for_plot, label='train_act_mse')\n",
    "plt.plot(valid_act_mses_for_plot, label='valid_act_mse')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(train_reward_mses_for_plot, label='train_reward_mse')\n",
    "plt.plot(valid_reward_mses_for_plot, label='valid_reward_mse')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(train_penalty_mses_for_plot, label='train_penalty_mse')\n",
    "plt.plot(valid_penalty_mses_for_plot, label='valid_penalty_mse')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_batch = {\n",
    "    feature_name: features_scalers[feature_name].inverse_transform(features_batch[feature_name], copy=True)\n",
    "    for feature_name in features_batch.keys()\n",
    "}\n",
    "targets_batch = {\n",
    "    target_name: targets_scalers[target_name].inverse_transform(targets_batch[target_name], copy=True)\n",
    "    for target_name in targets_batch.keys()\n",
    "}\n",
    "\n",
    "centerline = features_batch['centerline']\n",
    "centerline = centerline.reshape(len(centerline), -1, 2).cpu().numpy()\n",
    "\n",
    "traj_pred = trajectory_pred.reshape(len(trajectory_pred), -1, 2).cpu().numpy()\n",
    "\n",
    "right_bound = features_batch['right_bound']\n",
    "right_bound = right_bound.reshape(len(right_bound), -1, 2).cpu().numpy()\n",
    "\n",
    "left_bound = features_batch['left_bound']\n",
    "left_bound = left_bound.reshape(len(left_bound), -1, 2).cpu().numpy()\n",
    "\n",
    "trajectory = targets_batch['trajectory']\n",
    "traj = trajectory.reshape(len(trajectory), -1, 2).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):    \n",
    "    positions = traj[i]\n",
    "    positions_pred = traj_pred[i]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "    axes[0].plot(positions_pred[:, 0], positions_pred[:, 1], alpha=0.3, linewidth=5)\n",
    "    axes[0].plot(positions[:, 0], positions[:, 1], alpha=0.3, linewidth=5)\n",
    "    axes[0].plot(centerline[i, :, 0], centerline[i, :, 1], alpha=0.1, linewidth=5, color='red')\n",
    "    axes[0].scatter(right_bound[i, :, 0], right_bound[i, :, 1], color='gray', alpha=0.2)\n",
    "    axes[0].scatter(left_bound[i, :, 0], left_bound[i, :, 1], color='gray', alpha=0.2)\n",
    "    axes[0].set_aspect('equal')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, targets = valid_dataset[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_bound, right_bound = features['left_bound'], features['right_bound']\n",
    "left_bound = left_bound.reshape(-1, 2)\n",
    "right_bound = right_bound.reshape(-1, 2)\n",
    "plt.scatter(left_bound[:, 0], left_bound[:, 1], color='gray', alpha=0.2)\n",
    "plt.scatter(right_bound[:, 0], right_bound[:, 1], color='gray', alpha=0.2)\n",
    "\n",
    "centerline = features['centerline'].reshape(-1, 2)\n",
    "plt.scatter(centerline[:, 0], centerline[:, 1], color='r', alpha=0.2)\n",
    "\n",
    "trajectory = targets['trajectory']\n",
    "trajectory = trajectory.reshape(-1, 2)\n",
    "plt.scatter(trajectory[:, 0], trajectory[:, 1], color='blue', alpha=0.1)\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_batch = {\n",
    "    feature_name: features_scalers[feature_name].transform(features_batch[feature_name], copy=True)\n",
    "    for feature_name in features_batch.keys()\n",
    "}\n",
    "targets_batch = {\n",
    "    target_name: targets_scalers[target_name].transform(targets_batch[target_name], copy=True)\n",
    "    for target_name in targets_batch.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.1\n",
    "features, targets = valid_dataset[idx]\n",
    "num_steps = 4\n",
    "penalty_sigma = 0.1\n",
    "\n",
    "\n",
    "features = {\n",
    "    key: features_scalers[key].transform(torch.tensor(value, device=DEVICE, dtype=torch.float))\n",
    "    for key, value in features.items()\n",
    "}\n",
    "targets = {\n",
    "    key: targets_scalers[key].transform(torch.tensor(value, device=DEVICE, dtype=torch.float))\n",
    "    for key, value in targets.items()\n",
    "}\n",
    "\n",
    "\n",
    "contr_params = features['contr_params']\n",
    "num_contr_params = len(contr_params)\n",
    "step_directions = torch.cat(\n",
    "        [torch.zeros((1, num_contr_params))]\n",
    "        + [-step * torch.eye(num_contr_params) for step in reversed(range(1, num_steps+1))]\n",
    "        + [ step * torch.eye(num_contr_params) for step in range(1, num_steps+1)],\n",
    "    axis=0\n",
    ").to(DEVICE)\n",
    "\n",
    "batch_size = step_directions.shape[0]\n",
    "\n",
    "contr_params = contr_params[None] + eta * step_directions\n",
    "\n",
    "state = torch.tile(features['state'], (batch_size, 1))\n",
    "centerline = torch.tile(features['centerline'], (batch_size, 1))\n",
    "left_bound = torch.tile(features['left_bound'], (batch_size, 1))\n",
    "right_bound = torch.tile(features['right_bound'], (batch_size, 1))\n",
    "\n",
    "with torch.inference_mode():\n",
    "    preds = omniward_model(state, contr_params, centerline, None, None)\n",
    "    trajectory_pred, actuators_pred = preds['trajectory_pred'], preds['actuators_pred']\n",
    "\n",
    "trajectory_pred = targets_scalers['trajectory'].inverse_transform(trajectory_pred)\n",
    "reward_pred, penalty_pred = calc_reward_and_penalty(trajectory_pred, centerline, left_bound, right_bound, penalty_sigma=penalty_sigma)\n",
    "            \n",
    "\n",
    "traj = trajectory_pred.reshape(len(trajectory_pred), -1, 2).cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeds_and_deltas = targets_scalers['speeds_and_deltas'].inverse_transform(actuators_pred)[0].cpu()\n",
    "speeds_and_deltas_gt = targets_scalers['speeds_and_deltas'].inverse_transform(targets['speeds_and_deltas']).cpu()\n",
    "\n",
    "half = len(speeds_and_deltas) // 2\n",
    "plt.plot(speeds_and_deltas[:half])\n",
    "plt.plot(speeds_and_deltas_gt[:half])\n",
    "plt.show()\n",
    "plt.plot(speeds_and_deltas[half:])\n",
    "plt.plot(speeds_and_deltas_gt[half:]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps_for_grad = 4\n",
    "\n",
    "\n",
    "\n",
    "grad_contr_param = np.zeros(num_contr_params)\n",
    "x = eta * np.arange(-num_steps_for_grad, num_steps_for_grad+1)\n",
    "for contr_param_idx in range(num_contr_params):\n",
    "    rewards = []\n",
    "    penalties = []\n",
    "    for pred_step in range(2 * num_steps_for_grad):\n",
    "        pred_idx = 1 + pred_step * num_contr_params + contr_param_idx\n",
    "        rewards.append(float(reward_pred[pred_idx].cpu()))\n",
    "        penalties.append(float(penalty_pred[pred_idx].cpu()))\n",
    "        if pred_step == num_steps_for_grad:\n",
    "            rewards.append(float(reward_pred[0].cpu()))\n",
    "            penalties.append(float(penalty_pred[0].cpu()))\n",
    "        \n",
    "        \n",
    "\n",
    "    coeffs = np.polyfit(x, rewards, deg=1)\n",
    "    plt.plot(x, coeffs[0] * x + coeffs[1])\n",
    "    plt.plot(x, rewards)\n",
    "    plt.show()\n",
    "\n",
    "    coeffs = np.polyfit(x, penalties, deg=1)\n",
    "    plt.plot(x, coeffs[0] * x + coeffs[1])\n",
    "    plt.plot(x, penalties)\n",
    "    plt.show()\n",
    "\n",
    "    print('--------------------------------------------------------')\n",
    "\n",
    "    grad_contr_param[contr_param_idx] = coeffs[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dupa_idx in range(batch_size):\n",
    "    positions = traj[dupa_idx]\n",
    "    plt.scatter(positions[:, 0], positions[:, 1], alpha=0.1)\n",
    "# plt.gca().set_aspect('equal')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from racing_utils import GradientDriver\n",
    "\n",
    "\n",
    "centerline_direction = 1\n",
    "left_bound_direction = valid_dataset.traj_data[0]['bound_directions'][0]\n",
    "right_bound_direction = valid_dataset.traj_data[0]['bound_directions'][1]\n",
    "\n",
    "grad_driver = GradientDriver(\n",
    "    centerline=additional_data['centerline'][::centerline_direction],\n",
    "    num_steps_centerline=NUM_STEPS_CENTERLINE,\n",
    "\n",
    "    left_bound=valid_dataset.bounds[0][::left_bound_direction],\n",
    "    right_bound=valid_dataset.bounds[1],\n",
    "    num_steps_ahead_bound=NUM_STEPS_AHEAD_BOUND,\n",
    "\n",
    "    # Controller-related\n",
    "    init_contr_params=features['contr_params'],\n",
    "\n",
    "    # Model-related\n",
    "    omniward_model=omniward_model,\n",
    "    features_scalers=features_scalers,\n",
    "    targets_scalers=targets_scalers,\n",
    "\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "grad_driver.plan(\n",
    "    ranges=None,\n",
    "    yaw=row['yaw'],\n",
    "    pos_x=row['position'][0],\n",
    "    pos_y=row['position'][1],\n",
    "    linear_vel_x=row['v_x'],\n",
    "    linear_vel_y=row['v_y'],\n",
    "    angular_vel_z=row['omega'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_speeds = []\n",
    "first_speeds_pred = []\n",
    "first_deltas = []\n",
    "first_deltas_pred = []\n",
    "\n",
    "for act, act_pred in zip(actuators.cpu().numpy(), actuators_pred.detach().cpu().numpy()):\n",
    "    act = targets_scalers['speeds_and_deltas'].inverse_transform([act])[0]\n",
    "    act_pred = targets_scalers['speeds_and_deltas'].inverse_transform([act_pred])[0]\n",
    "    half = len(act) // 2\n",
    "    plt.plot(act[:half], alpha=0.5)\n",
    "    plt.plot(act_pred[:half])\n",
    "    first_speeds.append(act[0])\n",
    "    first_speeds_pred.append(act_pred[0])\n",
    "    first_deltas.append(act[half])\n",
    "    first_deltas_pred.append(act_pred[half])\n",
    "    plt.show()\n",
    "    plt.plot(act[half:], alpha=0.5)\n",
    "    plt.plot(act_pred[half:])\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a7bcfec8d1256c2cc01827278bf3ad7d8116bfe4d44c922aab01266a036f6cc2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('offline_rl': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
