{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Dict, Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "from data import RaceDataset\n",
    "\n",
    "from racing_utils.utils import closest_point_idx, cyclic_slice, determine_direction_of_bound, rotate_into_map_coord\n",
    "from racing_utils.torch_related import TensorStandardScaler, calc_progress_and_penalty, scale_batch_and_to_device\n",
    "from racing_utils.models import get_omniward_model\n",
    "\n",
    "from training import one_pass_through_data, process_batch, Phase, plot_losses_vs_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_STEPS_AHEAD_TRAJ = 150\n",
    "NUM_STEPS_AHEAD_ACT = 10\n",
    "NUM_STEPS_AHEAD_BOUND = 50\n",
    "NUM_STEPS_CENTERLINE = 300\n",
    "CENTERLINE_DECIMATION = 1\n",
    "\n",
    "DATASET_SUFFIX = '_tiny' # '_tiny', '', or '_large'\n",
    "\n",
    "DEVICE = 'cuda:0' # 'cuda:0' or 'cpu'\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "PROB_FLIP = 0.5  # Mini-augmentation\n",
    "\n",
    "# Model-related\n",
    "NUM_LAYERS = 3\n",
    "WIDTH_REDUCTION = 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducible\n",
    "# torch.backends.cudnn.determinstic = True\n",
    "# torch.backends.cudnn.benchmark = False\n",
    "#\n",
    "# Or fast\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpickled = pd.read_pickle('./data/train_large/2021-11-14_17_44_20.360805.pkl')\n",
    "one_race = unpickled['data']\n",
    "additional_data = unpickled['additional_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centerline = additional_data['centerline'][::CENTERLINE_DECIMATION]\n",
    "lookahead_distance = additional_data['lookahead_distance']\n",
    "speed_setpoint = additional_data['speed_setpoint']\n",
    "tire_force_max = additional_data['tire_force_max']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_race.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We need to figure out if the bounds go in the same direction as the car is driving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_position, end_position = one_race.loc[[0, NUM_STEPS_AHEAD_TRAJ], 'position']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = []\n",
    "bound_directions = []\n",
    "for csv_file in ['interior.csv', 'exterior.csv']:\n",
    "    bound = pd.read_csv(Path('./data') / csv_file, header=None).values\n",
    "    direction = determine_direction_of_bound(bound, start_position, end_position)\n",
    "    bound_directions.append(direction)\n",
    "    bound = bound[::direction]\n",
    "    bounds.append(bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_id = 120 + len(one_race) // 4\n",
    "row = one_race.iloc[row_id]\n",
    "\n",
    "position = row['position']\n",
    "yaw = row['yaw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_bound_indices = [closest_point_idx(position, bound) for bound in bounds] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bound_slices = [\n",
    "    cyclic_slice(bound, closest_idx, NUM_STEPS_AHEAD_BOUND)\n",
    "    for bound, closest_idx in zip(bounds, closest_bound_indices)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_positions = np.stack(one_race['position'].iloc[row_id:row_id+NUM_STEPS_AHEAD_TRAJ].values)\n",
    "\n",
    "for bound_slice in bound_slices:\n",
    "    plt.scatter(bound_slice[:, 0], bound_slice[:, 1])\n",
    "    \n",
    "plt.scatter(local_positions[:, 0], local_positions[:, 1])\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_positions = np.stack(one_race['position'].iloc[row_id:(row_id + NUM_STEPS_AHEAD_TRAJ)].values)\n",
    "\n",
    "closest_centerline_idx = closest_point_idx(local_positions[0], centerline)\n",
    "centerline_ahead = cyclic_slice(centerline, closest_centerline_idx, NUM_STEPS_CENTERLINE)\n",
    "centerline_ahead = rotate_into_map_coord(centerline_ahead - position, -yaw)\n",
    "plt.scatter(centerline_ahead[:, 0], centerline_ahead[:, 1], color='r', alpha=0.2)\n",
    "\n",
    "for bound_slice in bound_slices:\n",
    "    bound_slice = rotate_into_map_coord(bound_slice - position, -yaw)\n",
    "    plt.scatter(bound_slice[:, 0], bound_slice[:, 1], color='gray', alpha=0.2)\n",
    "        \n",
    "local_positions = rotate_into_map_coord(local_positions - position, -yaw)\n",
    "plt.scatter(local_positions[:, 0], local_positions[:, 1], alpha=0.1)\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = RaceDataset(\n",
    "    NUM_STEPS_AHEAD_TRAJ,\n",
    "    NUM_STEPS_AHEAD_ACT,\n",
    "    NUM_STEPS_AHEAD_BOUND,\n",
    "    NUM_STEPS_CENTERLINE,\n",
    "    f'./data/train{DATASET_SUFFIX}',\n",
    "    CENTERLINE_DECIMATION,\n",
    "    flip_prob=PROB_FLIP,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, targets = train_dataset[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_bound, right_bound = features['left_bound'], features['right_bound']\n",
    "left_bound = left_bound.reshape(-1, 2)\n",
    "right_bound = right_bound.reshape(-1, 2)\n",
    "plt.scatter(left_bound[:, 0], left_bound[:, 1], color='gray', alpha=0.2)\n",
    "plt.scatter(right_bound[:, 0], right_bound[:, 1], color='gray', alpha=0.2)\n",
    "\n",
    "centerline = features['centerline'].reshape(-1, 2)\n",
    "plt.scatter(centerline[:, 0], centerline[:, 1], color='r', alpha=0.2)\n",
    "\n",
    "trajectory = targets['trajectory']\n",
    "trajectory = trajectory.reshape(-1, 2)\n",
    "plt.scatter(trajectory[:, 0], trajectory[:, 1], color='blue', alpha=0.1)\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = data.DataLoader(train_dataset, BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Initilize the scalers\n",
    "features_scalers = {key: TensorStandardScaler(DEVICE) for key in features.keys()}\n",
    "targets_scalers = {key: TensorStandardScaler(DEVICE) for key in targets.keys()}\n",
    "\n",
    "# Do a partial fit of the scalers for both the input features, and the targets\n",
    "for features_batch, targets_batch in train_loader:\n",
    "    for feature_name in features.keys():\n",
    "        # TODO: not all features need rescaling, in particular: left_bound, right_bound\n",
    "        features_scalers[feature_name].partial_fit(features_batch[feature_name])\n",
    "\n",
    "    for target_name in targets.keys():\n",
    "        targets_scalers[target_name].partial_fit(targets_batch[target_name])\n",
    "\n",
    "# Move the numpy structures into a torch.tensor\n",
    "for feature_name in features.keys():\n",
    "        features_scalers[feature_name].tensorfy()\n",
    "\n",
    "for target_name in targets.keys():\n",
    "    targets_scalers[target_name].tensorfy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = RaceDataset(\n",
    "    NUM_STEPS_AHEAD_TRAJ,\n",
    "    NUM_STEPS_AHEAD_ACT,\n",
    "    NUM_STEPS_AHEAD_BOUND,\n",
    "    NUM_STEPS_CENTERLINE,\n",
    "    f'./data/valid{DATASET_SUFFIX}',\n",
    "    CENTERLINE_DECIMATION,\n",
    "    flip_prob=PROB_FLIP,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We're ready for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omniward_model = get_omniward_model(\n",
    "    NUM_LAYERS,\n",
    "    WIDTH_REDUCTION,\n",
    "    features,\n",
    "    targets,\n",
    "    DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omniward_model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We've got the model, now in order to train it we'll need two DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = data.DataLoader(train_dataset, BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "valid_loader = data.DataLoader(valid_dataset, BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The gammas below may seem a bit confusing but the idea is to have a sequence of coefficients that exponentially decrease for the actuators, AND a sequence of coefficients that exponentially decrease from the back for the trajectory\n",
    "\n",
    "This is because the trajectory is easy to guess at the beginning (all trajectories start in the (0, 0) point) whereas at the end it's more relevant for calculating the reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory_size = len(targets['trajectory'])\n",
    "actuators_size = len(targets['speeds_and_deltas'])\n",
    "\n",
    "trajectory_gamma = 0.99999\n",
    "actuators_gamma = 0.8\n",
    "\n",
    "trajectory_gammas = (trajectory_gamma ** np.r_[np.arange(trajectory_size // 2), np.arange(trajectory_size // 2)])\n",
    "trajectory_gammas = torch.from_numpy(trajectory_gammas[::-1].copy()).to(DEVICE)\n",
    "\n",
    "actuators_gammas = actuators_gamma ** np.r_[np.arange(actuators_size // 2), np.arange(actuators_size // 2)]\n",
    "actuators_gammas = torch.from_numpy(actuators_gammas).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penalty_sigma = 0.15\n",
    "\n",
    "optimizers = (\n",
    "    [5, torch.optim.Adam(omniward_model.parameters(), lr=1e-3)],\n",
    "    [15, torch.optim.Adam(omniward_model.parameters(), lr=1e-4)],\n",
    "    [5, torch.optim.Adam(omniward_model.parameters(), lr=1e-5)],\n",
    ")\n",
    "\n",
    "train_traj_mses_for_plot = []\n",
    "train_act_mses_for_plot = []\n",
    "train_progress_mses_for_plot = []\n",
    "train_penalty_mses_for_plot = []\n",
    "\n",
    "valid_traj_mses_for_plot = []\n",
    "valid_act_mses_for_plot = []\n",
    "valid_progress_mses_for_plot = []\n",
    "valid_penalty_mses_for_plot = []\n",
    "\n",
    "epoch = 0\n",
    "\n",
    "for num_epochs_per_optimizer_round, optimizer in optimizers:\n",
    "    print(f'Optimizer: {optimizer}')\n",
    "    for _ in range(num_epochs_per_optimizer_round):\n",
    "        \n",
    "        #            #\n",
    "        #  Training  #\n",
    "        #            #\n",
    "        one_pass_through_data(\n",
    "            model=omniward_model,\n",
    "            optimizer=optimizer,\n",
    "            phase=Phase.train,\n",
    "            data_loader=train_loader,\n",
    "            features_scalers=features_scalers,\n",
    "            targets_scalers=targets_scalers,\n",
    "            trajectory_gammas=trajectory_gammas,\n",
    "            actuators_gammas=actuators_gammas,\n",
    "            penalty_sigma=penalty_sigma,\n",
    "            traj_mses_for_plot=train_traj_mses_for_plot,\n",
    "            act_mses_for_plot=train_act_mses_for_plot,\n",
    "            progress_mses_for_plot=train_progress_mses_for_plot,\n",
    "            penalty_mses_for_plot=train_penalty_mses_for_plot,\n",
    "            epoch=epoch,\n",
    "            device=DEVICE,\n",
    "        )\n",
    "            \n",
    "        #              #\n",
    "        #  Validation  #\n",
    "        #              #\n",
    "        with torch.inference_mode():\n",
    "            one_pass_through_data(\n",
    "                model=omniward_model,\n",
    "                optimizer=None,\n",
    "                phase=Phase.valid,\n",
    "                data_loader=valid_loader,\n",
    "                features_scalers=features_scalers,\n",
    "                targets_scalers=targets_scalers,\n",
    "                trajectory_gammas=trajectory_gammas,\n",
    "                actuators_gammas=actuators_gammas,\n",
    "                penalty_sigma=penalty_sigma,\n",
    "                traj_mses_for_plot=valid_traj_mses_for_plot,\n",
    "                act_mses_for_plot=valid_act_mses_for_plot,\n",
    "                progress_mses_for_plot=valid_progress_mses_for_plot,\n",
    "                penalty_mses_for_plot=valid_penalty_mses_for_plot,\n",
    "                epoch=epoch,\n",
    "                device=DEVICE,\n",
    "            )\n",
    "\n",
    "        epoch += 1\n",
    "        print()\n",
    "\n",
    "    features_batch, targets_batch = next(iter(valid_loader))\n",
    "    with torch.inference_mode():\n",
    "        traj_mse, act_mse, progress_mse, penalty_mse = process_batch(\n",
    "            omniward_model,\n",
    "            features_scalers,\n",
    "            targets_scalers,\n",
    "            features_batch,\n",
    "            targets_batch,\n",
    "            trajectory_gammas,\n",
    "            actuators_gammas,\n",
    "            penalty_sigma,\n",
    "            DEVICE,\n",
    "            make_plots=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_batch, targets_batch = scale_batch_and_to_device(DEVICE, features_scalers, targets_scalers, features_batch, targets_batch)\n",
    "centerline = features_batch['centerline'].to(DEVICE)\n",
    "left_bound = features_batch['left_bound'].to(DEVICE)\n",
    "right_bound = features_batch['right_bound'].to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "with torch.inference_mode():\n",
    "    preds = omniward_model(**features_batch)\n",
    "    _1, _2 = preds['trajectory_pred'], preds['actuators_pred']\n",
    "    _ = targets_scalers['trajectory'].inverse_transform(_1)\n",
    "    calc_progress_and_penalty(_, centerline, left_bound, right_bound, penalty_sigma=penalty_sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses_vs_epochs(\n",
    "    train_traj_mses=train_traj_mses_for_plot,\n",
    "    valid_traj_mses=valid_traj_mses_for_plot,\n",
    "    train_act_mses=train_act_mses_for_plot,\n",
    "    valid_act_mses=valid_act_mses_for_plot,\n",
    "    train_progress_mses=train_progress_mses_for_plot,\n",
    "    valid_progress_mses=valid_progress_mses_for_plot,\n",
    "    train_penalty_mses=train_penalty_mses_for_plot,\n",
    "    valid_penalty_mses=valid_penalty_mses_for_plot,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    preds = omniward_model(**features_batch)\n",
    "    trajectory_pred, actuators_pred = preds['trajectory_pred'], preds['actuators_pred']\n",
    "    trajectory_pred = targets_scalers['trajectory'].inverse_transform(trajectory_pred)\n",
    "    progress_pred, penalty_pred = calc_progress_and_penalty(trajectory_pred, centerline, left_bound, right_bound, penalty_sigma=penalty_sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_batch = {\n",
    "    feature_name: features_scalers[feature_name].inverse_transform(features_batch[feature_name])\n",
    "    for feature_name in features_batch.keys()\n",
    "}\n",
    "targets_batch = {\n",
    "    target_name: targets_scalers[target_name].inverse_transform(targets_batch[target_name])\n",
    "    for target_name in targets_batch.keys()\n",
    "}\n",
    "\n",
    "centerline = features_batch['centerline']\n",
    "centerline = centerline.reshape(len(centerline), -1, 2).cpu().numpy()\n",
    "\n",
    "traj_pred = trajectory_pred.reshape(len(trajectory_pred), -1, 2).cpu().numpy()\n",
    "\n",
    "right_bound = features_batch['right_bound']\n",
    "right_bound = right_bound.reshape(len(right_bound), -1, 2).cpu().numpy()\n",
    "\n",
    "left_bound = features_batch['left_bound']\n",
    "left_bound = left_bound.reshape(len(left_bound), -1, 2).cpu().numpy()\n",
    "\n",
    "trajectory = targets_batch['trajectory']\n",
    "traj = trajectory.reshape(len(trajectory), -1, 2).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):    \n",
    "    positions = traj[i]\n",
    "    positions_pred = traj_pred[i]\n",
    "\n",
    "    plt.plot(positions_pred[:, 0], positions_pred[:, 1], alpha=0.3, linewidth=5)\n",
    "    plt.plot(positions[:, 0], positions[:, 1], alpha=0.3, linewidth=5)\n",
    "    plt.plot(centerline[i, :, 0], centerline[i, :, 1], alpha=0.1, linewidth=5, color='red')\n",
    "    plt.scatter(right_bound[i, :, 0], right_bound[i, :, 1], color='gray', alpha=0.2)\n",
    "    plt.scatter(left_bound[i, :, 0], left_bound[i, :, 1], color='gray', alpha=0.2)\n",
    "    plt.gca().set_aspect('equal')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, targets = valid_dataset[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_bound, right_bound = features['left_bound'], features['right_bound']\n",
    "left_bound = left_bound.reshape(-1, 2)\n",
    "right_bound = right_bound.reshape(-1, 2)\n",
    "plt.scatter(left_bound[:, 0], left_bound[:, 1], color='gray', alpha=0.2)\n",
    "plt.scatter(right_bound[:, 0], right_bound[:, 1], color='gray', alpha=0.2)\n",
    "\n",
    "centerline = features['centerline'].reshape(-1, 2)\n",
    "plt.scatter(centerline[:, 0], centerline[:, 1], color='r', alpha=0.2)\n",
    "\n",
    "trajectory = targets['trajectory']\n",
    "trajectory = trajectory.reshape(-1, 2)\n",
    "plt.scatter(trajectory[:, 0], trajectory[:, 1], color='blue', alpha=0.1)\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_batch = {\n",
    "    feature_name: features_scalers[feature_name].transform(features_batch[feature_name])\n",
    "    for feature_name in features_batch.keys()\n",
    "}\n",
    "targets_batch = {\n",
    "    target_name: targets_scalers[target_name].transform(targets_batch[target_name])\n",
    "    for target_name in targets_batch.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeds_and_deltas = targets_scalers['speeds_and_deltas'].inverse_transform(actuators_pred)[0].cpu()\n",
    "speeds_and_deltas_gt = targets_scalers['speeds_and_deltas'].inverse_transform(targets_batch['speeds_and_deltas'])[0].cpu()\n",
    "\n",
    "half = len(speeds_and_deltas) // 2\n",
    "plt.plot(speeds_and_deltas[:half])\n",
    "plt.plot(speeds_and_deltas_gt[:half])\n",
    "plt.show()\n",
    "plt.plot(speeds_and_deltas[half:])\n",
    "plt.plot(speeds_and_deltas_gt[half:]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.05\n",
    "num_steps_for_grad = 4\n",
    "num_contr_params = features_batch['contr_params'].shape[1]\n",
    "\n",
    "\n",
    "grad_contr_param = np.zeros(num_contr_params)\n",
    "x = eta * np.arange(-num_steps_for_grad, num_steps_for_grad+1)\n",
    "for contr_param_idx in range(num_contr_params):\n",
    "    progress = []\n",
    "    penalties = []\n",
    "    for pred_step in range(2 * num_steps_for_grad):\n",
    "        pred_idx = 1 + pred_step * num_contr_params + contr_param_idx\n",
    "        progress.append(float(progress_pred[pred_idx].cpu()))\n",
    "        penalties.append(float(penalty_pred[pred_idx].cpu()))\n",
    "        if pred_step == num_steps_for_grad:\n",
    "            progress.append(float(progress_pred[0].cpu()))\n",
    "            penalties.append(float(penalty_pred[0].cpu()))\n",
    "        \n",
    "    coeffs = np.polyfit(x, progress, deg=1)\n",
    "    plt.plot(x, coeffs[0] * x + coeffs[1])\n",
    "    plt.plot(x, progress)\n",
    "    plt.show()\n",
    "\n",
    "    coeffs = np.polyfit(x, penalties, deg=1)\n",
    "    plt.plot(x, coeffs[0] * x + coeffs[1])\n",
    "    plt.plot(x, penalties)\n",
    "    plt.show()\n",
    "\n",
    "    print(80 * '-')\n",
    "\n",
    "    grad_contr_param[contr_param_idx] = coeffs[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = features_batch['contr_params'].shape[0]\n",
    "\n",
    "for idx in range(batch_size):\n",
    "    positions = traj[idx]\n",
    "    plt.scatter(positions[:, 0], positions[:, 1], alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omniward_model_cpu = omniward_model.to('cpu')\n",
    "\n",
    "\n",
    "for features_scaler in features_scalers.values():\n",
    "    features_scaler.to('cpu')\n",
    "\n",
    "for targets_scaler in targets_scalers.values():\n",
    "    targets_scaler.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contr_param_limits = train_dataset.determine_limits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from racing_utils import GradientDriver\n",
    "\n",
    "\n",
    "centerline_direction = 1\n",
    "left_bound_direction = valid_dataset.traj_data[0]['bound_directions'][0]\n",
    "right_bound_direction = valid_dataset.traj_data[0]['bound_directions'][1]\n",
    "\n",
    "grad_driver = GradientDriver(\n",
    "    centerline=additional_data['centerline'][::centerline_direction],\n",
    "    num_steps_centerline=NUM_STEPS_CENTERLINE,\n",
    "\n",
    "    left_bound=valid_dataset.bounds[0][::left_bound_direction],\n",
    "    right_bound=valid_dataset.bounds[1],\n",
    "    num_steps_ahead_bound=NUM_STEPS_AHEAD_BOUND,\n",
    "\n",
    "    # Controller-related\n",
    "    init_contr_params=np.r_[additional_data['lookahead_distance'], additional_data['speed_setpoint'], additional_data['tire_force_max']],\n",
    "\n",
    "    # Model-related\n",
    "    omniward_model=omniward_model_cpu,\n",
    "    features_scalers=features_scalers,\n",
    "    targets_scalers=targets_scalers,\n",
    "\n",
    "    # Gradient-related\n",
    "    eta=0.1,\n",
    "    num_steps_for_grad=4,\n",
    "    penalty_sigma=0.3,\n",
    "    penalty_scale_coeff=-0.9,\n",
    "    contr_params_limits=contr_param_limits,\n",
    "\n",
    "    device='cpu',\n",
    ")\n",
    "\n",
    "grad_driver.plan(\n",
    "    ranges=None,\n",
    "    yaw=row['yaw'],\n",
    "    pos_x=row['position'][0],\n",
    "    pos_y=row['position'][1],\n",
    "    linear_vel_x=row['v_x'],\n",
    "    linear_vel_y=row['v_y'],\n",
    "    angular_vel_z=row['omega'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(grad_driver, 'grad_driver.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_idx, end_idx = 100, 400\n",
    "\n",
    "speed_preds = []\n",
    "speed_ground_truths = []\n",
    "steer_preds = []\n",
    "steer_ground_truths = []\n",
    "\n",
    "\n",
    "for row_idx, row in one_race.iterrows():\n",
    "    if row_idx < start_idx:\n",
    "        continue\n",
    "\n",
    "    speed, steer = grad_driver.plan(\n",
    "        ranges=None,\n",
    "        yaw=row['yaw'],\n",
    "        pos_x=row['position'][0],\n",
    "        pos_y=row['position'][1],\n",
    "        linear_vel_x=row['v_x'],\n",
    "        linear_vel_y=row['v_y'],\n",
    "        angular_vel_z=row['omega'],\n",
    "    )\n",
    "    speed_gt = row['speed_actuator']\n",
    "    steer_gt = row['delta']\n",
    "\n",
    "    speed_preds.append(speed)\n",
    "    speed_ground_truths.append(speed_gt)\n",
    "    steer_preds.append(steer)\n",
    "    steer_ground_truths.append(steer_gt)\n",
    "\n",
    "    if row_idx == end_idx:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(steer_ground_truths, steer_preds);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(speed_ground_truths, speed_preds);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a7bcfec8d1256c2cc01827278bf3ad7d8116bfe4d44c922aab01266a036f6cc2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('offline_rl': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
